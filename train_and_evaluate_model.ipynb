{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.layers import Dense, Input, Embedding, SpatialDropout1D, Dropout, Activation, BatchNormalization, \\\n",
    "    concatenate, Bidirectional, Conv1D, GlobalMaxPooling1D, CuDNNGRU\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from fastText import load_model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping, Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from collections import defaultdict\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from readability import Readability\n",
    "from readability.exceptions import ReadabilityException\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "def get_session(gpu_fraction=1.0):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    return tf.Session(config=config)\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "\n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(allow_growth=True))\n",
    "\n",
    "KTF.set_session(get_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "RUNS = 10\n",
    "GUARDIAN = True #True to run experiments on user comments from TheGuardian.com. False to run them on product reviews from Amazon.com\n",
    "COLUMN = 'comment_text'\n",
    "VOCABULARY_SIZE = 200000\n",
    "MAX_SEQUENCE_LENGTH = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GUARDIAN:\n",
    "    ft_model = load_model('guardian-300.bin')\n",
    "else:\n",
    "    ft_model = load_model('amazon-300.bin')\n",
    "EMBEDDING_DIM = ft_model.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_readability(comment_words):\n",
    "    try:\n",
    "        return Readability(comment_words).ari().score\n",
    "    except ReadabilityException:\n",
    "        return 1\n",
    "    \n",
    "def enrich_df_with_features(df):\n",
    "    df['ARI'] = df['comment_text'].apply(metric_readability)\n",
    "    df['comment_length'] = df['comment_text'].apply(len)\n",
    "    df_user_info = df.groupby('comment_author_id').mean()[['ARI','comment_length','upvotes','children']].add_suffix('_AVG').reset_index()\n",
    "    df_user_info = df_user_info[['comment_author_id', 'ARI_AVG','comment_length_AVG','upvotes_AVG','children_AVG']]\n",
    "    return df_user_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    # transform to lowercase characters\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    if 'http' in s or 'www' in s:\n",
    "        s = 'url'\n",
    "    # Isolate punctuation\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\-\\\\\\/\\,])', r' \\1 ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|\\n])', ' ', s)\n",
    "    return s\n",
    "\n",
    "def probabilities_to_classes(a):\n",
    "    if a[0]>a[1]:\n",
    "        return [1,0]\n",
    "    else:\n",
    "        return [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(category,percentage='10',fraction=1.0):\n",
    "    if GUARDIAN:\n",
    "        top = pd.read_csv('comments_top_and_bottom_'+category+'_'+percentage+'percent.csv', usecols=['comment_id', 'comment_author_id', 'comment_text', 'class','children', 'upvotes'])\n",
    "        top = top[top['class']==1]\n",
    "        bottom = pd.read_csv('comments_top_and_bottom_'+category+'_'+percentage+'percent.csv', usecols=['comment_id', 'comment_author_id', 'comment_text', 'class','children', 'upvotes'])\n",
    "        bottom = bottom[bottom['class']==0]\n",
    "    else:\n",
    "        top = pd.read_csv('reviews_books_top_'+percentage+'percent.csv')\n",
    "        top.columns = [COLUMN]\n",
    "        bottom = pd.read_csv('reviews_books_bottom_'+percentage+'percent.csv')\n",
    "        bottom.columns = [COLUMN]\n",
    "\n",
    "    top['comment_length'] = top['comment_text'].astype(str).apply(len)\n",
    "    bottom['comment_length'] = bottom['comment_text'].astype(str).apply(len)\n",
    "    \n",
    "    #If there are more than MAX_SEQUENCE_LENGTH tokens, use only the last MAX_SEQUENCE_LENGTH tokens of the comment\n",
    "    top[COLUMN] = top[COLUMN].apply(lambda x: ' '.join(str(x).split()[-MAX_SEQUENCE_LENGTH:]))\n",
    "    bottom[COLUMN] = bottom[COLUMN].apply(lambda x: ' '.join(str(x).split()[-MAX_SEQUENCE_LENGTH:]))\n",
    "    \n",
    "    top['top'] = True\n",
    "    top['bottom'] = False\n",
    "    bottom['top'] = False\n",
    "    bottom['bottom'] = True\n",
    "    \n",
    "    top_and_bottom = top.append(bottom)\n",
    "    top_and_bottom = top_and_bottom.sample(frac=fraction, random_state=42)\n",
    "    top_and_bottom[COLUMN] = top_and_bottom[COLUMN].astype(str)\n",
    "    return top_and_bottom\n",
    "\n",
    "def gru_model(embedding_layer):\n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') \n",
    "    embedded_sequences = embedding_layer(inp)\n",
    "    x = SpatialDropout1D(DROPOUT)(embedded_sequences)\n",
    "    x = Bidirectional(CuDNNGRU(32, return_sequences=False))(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    x = Dense(2, activation=\"softmax\")(x)\n",
    "    return Model(inputs=inp, outputs=x)\n",
    "\n",
    "def cnn_model(embedding_layer):\n",
    "    conv_filters = 128\n",
    "    \n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') \n",
    "    emb = embedding_layer(inp)\n",
    "\n",
    "    # Specify each convolution layer and their kernel size i.e. n-grams \n",
    "    conv1_1 = Conv1D(filters=conv_filters, kernel_size=3)(emb)\n",
    "    btch1_1 = BatchNormalization()(conv1_1)\n",
    "    drp1_1  = Dropout(0.2)(btch1_1)\n",
    "    actv1_1 = Activation('relu')(drp1_1)\n",
    "    glmp1_1 = GlobalMaxPooling1D()(actv1_1)\n",
    "\n",
    "    conv1_2 = Conv1D(filters=conv_filters, kernel_size=4)(emb)\n",
    "    btch1_2 = BatchNormalization()(conv1_2)\n",
    "    drp1_2  = Dropout(0.2)(btch1_2)\n",
    "    actv1_2 = Activation('relu')(drp1_2)\n",
    "    glmp1_2 = GlobalMaxPooling1D()(actv1_2)\n",
    "\n",
    "    conv1_3 = Conv1D(filters=conv_filters, kernel_size=5)(emb)\n",
    "    btch1_3 = BatchNormalization()(conv1_3)\n",
    "    drp1_3  = Dropout(0.2)(btch1_3)\n",
    "    actv1_3 = Activation('relu')(drp1_3)\n",
    "    glmp1_3 = GlobalMaxPooling1D()(actv1_3)\n",
    "\n",
    "    conv1_4 = Conv1D(filters=conv_filters, kernel_size=6)(emb)\n",
    "    btch1_4 = BatchNormalization()(conv1_4)\n",
    "    drp1_4  = Dropout(0.2)(btch1_4)\n",
    "    actv1_4 = Activation('relu')(drp1_4)\n",
    "    glmp1_4 = GlobalMaxPooling1D()(actv1_4)\n",
    "\n",
    "    # Gather all convolution layers\n",
    "    cnct = concatenate([glmp1_1, glmp1_2, glmp1_3, glmp1_4], axis=1)\n",
    "    drp1 = Dropout(0.2)(cnct)\n",
    "\n",
    "    dns1  = Dense(32, activation='relu')(drp1)\n",
    "    btch1 = BatchNormalization()(dns1)\n",
    "    drp2  = Dropout(0.2)(btch1)\n",
    "\n",
    "    out = Dense(2, activation='softmax')(drp2)\n",
    "    x = out\n",
    "    return Model(inputs=inp, outputs=x)\n",
    "\n",
    "def evaluate_model(x_test_text, model_type, x_train, y_train, x_val, y_val, x_test, y_test, embedding_layer):\n",
    "    if model_type == 'GRU':\n",
    "        model = gru_model(embedding_layer)\n",
    "    else:\n",
    "        model = cnn_model(embedding_layer)\n",
    "\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='auto')\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size = BATCH_SIZE, verbose=0, callbacks=[earlyStopping], epochs=EPOCHS)\n",
    "    y_test_pred = model.predict(x_test, verbose=0, batch_size=512)\n",
    "\n",
    "    yc = np.asarray(y_test_pred)\n",
    "    y_test_pred_binary = np.apply_along_axis(probabilities_to_classes, 1, yc)\n",
    "    score = accuracy_score(y_test, y_test_pred_binary)\n",
    "\n",
    "    ex_dict = {\n",
    "    'y_true': y_test[:,0],\n",
    "    'y_pred': y_test_pred_binary[:,0],\n",
    "    'x_test_text': x_test_text\n",
    "    }\n",
    "    columns = ['y_true', 'y_pred', 'x_test_text']\n",
    "    index = np.arange(1, len(y_test[:,0])+1, 1)\n",
    "\n",
    "    # Passing a dictionary to create a dataframe\n",
    "    # key: column name\n",
    "    # value: series of values\n",
    "    test_dataset = pd.DataFrame(ex_dict, columns=columns, index=index)\n",
    "    test_dataset.to_csv('test_dataset_predictions.csv', index=False)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(finished_runs = 10, model_types=['GRU','CNN']):\n",
    "    print(str(finished_runs)+' finished_run(s)\\n')\n",
    "    with open('logfile.txt', 'a') as the_file:\n",
    "        the_file.write(str(finished_runs)+' finished_run(s)\\n')\n",
    "        for category in ['upvotes', 'replies']:\n",
    "            the_file.write(category+'\\n')\n",
    "            for percentage in ['10','25','50']:\n",
    "                the_file.write(percentage+'\\n')\n",
    "                for model_type in model_types:\n",
    "                    the_file.write(str(model)+'\\n')\n",
    "                    tmp = 0\n",
    "                    for run in range(1,finished_runs+1):\n",
    "                        tmp += results[(run,category,percentage,model)]\n",
    "                        the_file.write(str(results[(run,category,percentage,model_type)])+'\\n')\n",
    "                    print(\"Accuracy: {:.3f}\".format(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_for_t_test(finished_runs = 10, model_types=['GRU','CNN']):\n",
    "    print(str(finished_runs)+' finished_run(s)\\n')\n",
    "    with open('logfile_t_test.txt', 'a') as the_file:\n",
    "        the_file.write(str(finished_runs)+' finished_run(s)\\n')\n",
    "        for category in ['upvotes', 'replies']:\n",
    "            the_file.write(category+'\\n')\n",
    "            for percentage in ['10','25','50']:\n",
    "                the_file.write(percentage+'\\n')\n",
    "                for model_type in model_types:\n",
    "                    the_file.write(str(model)+'\\n')\n",
    "                    tmp = 0\n",
    "                    the_file.write(\"<-c(\")\n",
    "                    for run in range(1,finished_runs+1):\n",
    "                        the_file.write(\"{:.4f}\".format(results[(run,category,percentage,model_type)])+\", \")\n",
    "                    the_file.write(\")\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiments with GRU and CNN models\n",
    "results = defaultdict(lambda: 0.0)\n",
    "if GUARDIAN:\n",
    "    categories = ['upvotes', 'replies']\n",
    "else:\n",
    "    categories = ['upvotes']\n",
    "for run in range(1,RUNS+1):\n",
    "    for category in categories:\n",
    "        #we always use 10% of the \"top/flop 10% dataset\" as test data\n",
    "        test_data = load_data(category,'10', fraction=0.1)\n",
    "        test_data.to_csv(str(GUARDIAN)+''+category+''+str(run)+'test_data.csv', index=False)\n",
    "        \n",
    "\n",
    "        for percentage in ['10','25','50']:\n",
    "            #we use varying training datasets (either \"top/flop 10% dataset\", \"top/flop 25% dataset\", or \"top/flop 50% dataset\")\n",
    "            train_data = load_data(category,percentage, fraction =1.0)\n",
    "            #we ensure that there is no overlap of training and test data by removing all test samples from the training data\n",
    "            train_data = pd.concat([train_data, test_data, test_data]).drop_duplicates(keep=False)\n",
    "            #the tokenizer uses the comment texts from both training and test data to define the vocabulary\n",
    "            texts = pd.concat([train_data, test_data])[COLUMN].values\n",
    "            train_data, val_data = train_test_split(train_data, test_size=20/80)\n",
    "            \n",
    "            x_train = train_data[COLUMN].values\n",
    "            x_val = val_data[COLUMN].values\n",
    "            x_test = test_data[COLUMN].values\n",
    "\n",
    "            y_train = train_data[['top', 'bottom']].values\n",
    "            y_val = val_data[['top', 'bottom']].values\n",
    "            y_test = test_data[['top', 'bottom']].values\n",
    "            \n",
    "            tokenizer = Tokenizer(num_words=VOCABULARY_SIZE)\n",
    "            tokenizer.fit_on_texts(texts)\n",
    "            \n",
    "            x_test_text = x_test\n",
    "            x_train = tokenizer.texts_to_sequences(x_train)\n",
    "            x_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)            \n",
    "            x_val = tokenizer.texts_to_sequences(x_val)\n",
    "            x_val = pad_sequences(x_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "            x_test = tokenizer.texts_to_sequences(x_test)\n",
    "            x_test = pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "            \n",
    "            word_index = tokenizer.word_index\n",
    "            num_words = min(VOCABULARY_SIZE, len(word_index)) + 1\n",
    "            embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "            for word, i in word_index.items():\n",
    "                if i > VOCABULARY_SIZE:\n",
    "                    continue\n",
    "                embedding_matrix[i] = ft_model.get_word_vector(word.lower()).astype('float32')\n",
    "\n",
    "            embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "            \n",
    "            for model_type in ['GRU','CNN']:\n",
    "                results[(run,category,percentage,model)] = evaluate_model(x_test_text, model_type, x_train, y_train, x_val, y_val, x_test, y_test, embedding_layer)\n",
    "            print_results(run)\n",
    "            print_results_for_t_test(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiments with Logistic Regression model\n",
    "results = defaultdict(lambda: 0.0)\n",
    "\n",
    "if GUARDIAN:\n",
    "    categories = ['upvotes', 'replies']\n",
    "else:\n",
    "    categories = ['upvotes']\n",
    "for run in range(1,RUNS+1):\n",
    "    for category in categories:\n",
    "        for percentage in ['10','25','50']:\n",
    "            #we always use 10% of the \"top/flop 10% dataset\" as test data\n",
    "            test_data_tmp = load_data(category,'10', fraction=0.1) \n",
    "            #we use varying training datasets (either \"top/flop 10% dataset\", \"top/flop 25% dataset\", or \"top/flop 50% dataset\")\n",
    "            train_data = load_data(category,percentage, fraction =1.0)            \n",
    "            #we ensure that there is no overlap of training and test data by removing all test samples from the training data\n",
    "            train_data = pd.concat([train_data, test_data_tmp, test_data_tmp]).drop_duplicates(keep=False) \n",
    "\n",
    "            test_data_tmp['ARI'] = test_data_tmp['comment_text'].apply(metric_readability)\n",
    "            test_data_tmp['comment_length'] = test_data_tmp['comment_text'].apply(len)\n",
    "            df_user_info = enrich_df_with_features(train_data)\n",
    "            train_data['ARI'] = train_data['comment_text'].apply(metric_readability)\n",
    "            train_data['comment_length'] = train_data['comment_text'].apply(len)\n",
    "            train_data = pd.merge(train_data, df_user_info, on='comment_author_id', how='outer')\n",
    "            train_data.dropna(subset=['comment_text'], inplace= True)\n",
    "            test_data = pd.merge(test_data_tmp, df_user_info, on='comment_author_id', how='outer')\n",
    "            test_data.dropna(subset=['comment_text', 'top'], inplace= True)\n",
    "            train_data, val_data = train_test_split(train_data, test_size=20/80)\n",
    "            test_data.fillna(0, inplace=True)\n",
    "            \n",
    "            #x_train = train_data[['ARI','comment_length','upvotes_AVG','children_AVG','ARI_AVG', 'comment_length_AVG']].values#.reshape(-1, 1)\n",
    "            #x_val = val_data[['ARI','comment_length','upvotes_AVG','children_AVG','ARI_AVG', 'comment_length_AVG']].values#.reshape(-1, 1)\n",
    "            #x_test = test_data[['ARI','comment_length','upvotes_AVG','children_AVG','ARI_AVG', 'comment_length_AVG']].values#.reshape(-1, 1)\n",
    "            x_train = train_data[['comment_length']].values.reshape(-1, 1)\n",
    "            x_val = val_data[['comment_length']].values.reshape(-1, 1)\n",
    "            x_test = test_data[['comment_length']].values.reshape(-1, 1)\n",
    "            \n",
    "            y_train = train_data[['top']].values.ravel()\n",
    "            y_val = val_data[['top']].values.ravel()\n",
    "            y_test = test_data[['top']].astype(bool).values.ravel()\n",
    "            \n",
    "            for model_type in ['LogisticRegression']:\n",
    "                logreg = LogisticRegression()\n",
    "                logreg.fit(x_train, y_train)\n",
    "                y_pred = logreg.predict(x_test)\n",
    "                score = logreg.score(x_test, y_test)\n",
    "                print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(score))\n",
    "                results[(run,category,percentage,model_type)] = score\n",
    "            print_results(run, model_types=['LogisticRegression'])\n",
    "            print_results_for_t_test(run, model_types=['LogisticRegression'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
